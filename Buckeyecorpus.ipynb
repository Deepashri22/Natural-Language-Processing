{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03acd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def merge_text_files(input_dir, output_file):\n",
    "    with open(output_file, 'w') as output:\n",
    "        for file_name in os.listdir(input_dir):\n",
    "            if file_name.endswith('.txt'):  \n",
    "                with open(os.path.join(input_dir, file_name), 'r') as input_file:\n",
    "                    output.write(input_file.read())\n",
    "                    output.write('\\n')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ac8427",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'C:/Users/kalap/Downloads/Total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ae1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'speech_words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "041f7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_text_files(input_dir, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c51151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Clean text saved to: output.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    patterns = [\n",
    "        r'<SIL>',          \n",
    "        r'<NOISE>',        \n",
    "        r'<IVER>',        \n",
    "        r'<VOCNOISE>',     \n",
    "        r'<EXT-Ive>',      \n",
    "        r'<HES-I>',        \n",
    "        r'<EXCLUDE-name>'  \n",
    "    ]\n",
    "    \n",
    "    \n",
    "    regex_patterns = [re.compile(pattern) for pattern in patterns]\n",
    "    \n",
    "\n",
    "    for pattern in regex_patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    \n",
    "   \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "input_file_path = \"speech_words.txt\"\n",
    "output_file_path = \"output.txt\"\n",
    "\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    input_text = file.read()\n",
    "\n",
    "\n",
    "clean_text = preprocess_text(input_text)\n",
    "\n",
    "with open(output_file_path, \"w\") as file:\n",
    "    file.write(clean_text)\n",
    "\n",
    "print(\"Preprocessing complete. Clean text saved to:\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f807c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cmudict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15e875c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\kalap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\cmudict.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88655bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "arpabet = nltk.corpus.cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0398963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(text, num_sentences=50):\n",
    "    sentences = nltk.sent_tokenize(text) \n",
    "    sentences = sentences[:num_sentences]\n",
    "    words = [word.lower() for sentence in sentences for word in nltk.word_tokenize(sentence)]    \n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    \n",
    "   \n",
    "    unique_words = list(set(words))\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a2a4c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_arpabet(word):\n",
    "    if word in arpabet:\n",
    "        return arpabet[word][0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db892e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_text = 'output.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "233bac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_text, \"r\") as file:\n",
    "    processed_text = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b17835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = extract_words(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68808a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = unique_words[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ae4fc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "informed: IH0 N F AO1 R M D\n",
      "pretty: P R IH1 T IY0\n",
      "onto: AA1 N T UW0\n",
      "i: AY1\n",
      "felt: F EH1 L T\n",
      "learning: L ER1 N IH0 NG\n",
      "program: P R OW1 G R AE2 M\n",
      "sporting: S P AO1 R T IH0 NG\n",
      "read: R EH1 D\n",
      "general: JH EH1 N ER0 AH0 L\n",
      "feel: F IY1 L\n",
      "let: L EH1 T\n",
      "attached: AH0 T AE1 CH T\n",
      "care: K EH1 R\n",
      "or: AO1 R\n",
      "world: W ER1 L D\n",
      "still: S T IH1 L\n",
      "african: AE1 F R AH0 K AH0 N\n",
      "she: SH IY1\n",
      "couple: K AH1 P AH0 L\n",
      "swimsuit: S W IH1 M S UW2 T\n",
      "house: HH AW1 S\n",
      "scout: S K AW1 T\n",
      "else: EH1 L S\n",
      "promoting: P R AH0 M OW1 T IH0 NG\n",
      "treadmill: T R EH1 D M IH2 L\n",
      "what: W AH1 T\n",
      "customs: K AH1 S T AH0 M Z\n",
      "stories: S T AO1 R IY0 Z\n",
      "happen: HH AE1 P AH0 N\n",
      "we: W IY1\n",
      "recognition: R EH2 K AH0 G N IH1 SH AH0 N\n",
      "parent: P EH1 R AH0 N T\n",
      "often: AO1 F AH0 N\n",
      "forward: F AO1 R W ER0 D\n",
      "laugh: L AE1 F\n",
      "sister: S IH1 S T ER0\n",
      "amount: AH0 M AW1 N T\n",
      "myself: M AY2 S EH1 L F\n",
      "computer: K AH0 M P Y UW1 T ER0\n",
      "wow: W AW1\n",
      "joey: JH OW1 IY0\n",
      "houses: HH AW1 S AH0 Z\n",
      "going: G OW1 IH0 NG\n",
      "hilliard: HH IH1 L Y AA0 R D\n",
      "news: N UW1 Z\n",
      "lake: L EY1 K\n",
      "people: P IY1 P AH0 L\n",
      "temper: T EH1 M P ER0\n"
     ]
    }
   ],
   "source": [
    "arpabet_words = {}\n",
    "for word in unique_words:\n",
    "    arpabet_transcription = convert_to_arpabet(word)\n",
    "    if arpabet_transcription:\n",
    "        arpabet_words[word] = arpabet_transcription\n",
    "\n",
    "for word, transcription in arpabet_words.items():\n",
    "    print(f\"{word}: {' '.join(transcription)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ace88d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
